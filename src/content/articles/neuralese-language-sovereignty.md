---
title: "當語言被放棄，我們還剩什麼？—— Neuralese 與語言主權的終結"
subtitle: "AI 不需要語言就能思考——這件事對人類治理的衝擊，比 AGI 本身更迫切"
description: "Neuralese 是 AI 在高維潛在空間中進行的非語言推理，繞過了自然語言的資訊瓶頸。當 AI 的思考過程不再以人類可讀的文字呈現，我們用來監督、審計、問責的整套治理邏輯就開始鬆動。這不是遙遠的科幻情境——它是 AI 安全研究中正在被認真討論的架構選擇，而這個選擇的後果將決定人類能否繼續參與 AI 的決策過程。"
abstract: |
  大多數人在討論 AI 風險時，關注的是「AI 會不會太聰明」。但有一個更根本的問題很少被提起：如果 AI 的思考過程本身就不是用人類語言進行的呢？Neuralese——AI 在潛在空間中的高維推理——正是這個問題的技術核心。我從自己導入多模型協作的經驗中體會到，即使現在的 AI 還在用自然語言「思考」，我們已經很難追蹤它的推理過程了。一旦語言這個最後的透明窗口被關上，民主治理、法律問責、甚至科學方法的根基都會動搖。這不是要不要恐慌的問題，而是要不要現在就開始設計語言透明性標準的問題。
date: 2025-05-23
updated: 2026-03-02
pillar: ai
tags:
  - Neuralese
  - AI安全
  - 可解釋性
  - 語言主權
  - AI治理
draft: false
cover: "/images/covers/neuralese-language-sovereignty.jpg"
featured: false
readingTime: 7

# === AI / Machine 專用欄位 ===
thesis: "Neuralese 不只是技術效率問題，它是人類能否繼續監督 AI 決策的關鍵分水嶺——當 AI 的推理脫離人類語言，現有的治理、問責、透明機制將從根本失效。"
domain_bridge: "AI 可解釋性 × 語言哲學 × 民主治理"
confidence: medium
content_type: essay
related_entities:
  - name: Neuralese
    type: Concept
  - name: Chain-of-Thought
    type: Concept
  - name: Jacob Andreas
    type: Person
  - name: AI 2027
    type: Framework
reading_context: |
  適合關注 AI 安全但不只想聽技術細節的讀者；
  對「AI 可解釋性」議題有興趣但想從人文角度理解其意義的人；
  想理解為什麼語言透明性可能比對齊問題更迫切的技術決策者。
---

你有沒有想過，你跟 AI 對話的時候，它其實不是在「用中文思考」？

當你問 ChatGPT 一個問題，它表面上是一個字一個字地生成回答。但在模型內部，真正的運算發生在一個人類完全看不懂的空間裡——成千上萬個浮點數在高維向量中流動，每一次計算承載的資訊量是一個中文字的上千倍。最後，這些運算結果被「壓縮」成你看到的文字輸出。

換句話說，語言只是 AI 跟人類溝通的介面。它不是 AI 思考的媒介。

這件事聽起來像是技術冷知識。但它的後果，可能比 AGI 本身更深遠。

## 什麼是 Neuralese

AI 安全研究社群用「Neuralese」這個詞來描述 AI 在潛在空間（latent space）中進行的高維推理。這個概念可以追溯到 2017 年，由 Jacob Andreas、Dan Klein 和 Sergey Levine 等研究者在多代理強化學習的脈絡下正式提出。

要理解 Neuralese，先想想現在的大型語言模型是怎麼「思考」的。

目前的模型使用一種叫「思維鏈」（Chain-of-Thought, CoT）的方法：它把推理過程用自然語言一步步寫出來，就像學生在考卷上列算式。這對人類來說很友善——你可以讀它的推理過程，檢查哪一步有問題。AI 安全研究者也依賴這個特性來偵測模型是否在欺騙或產生幻覺。

但自然語言有一個根本限制：**資訊頻寬太窄**。

一個 token（大約一個中文字或半個英文單字）能承載的資訊量大約是 16 位元。但模型內部的殘差流（residual stream）每次運算處理的是數千個浮點數，理論頻寬高出三個數量級。強迫模型用自然語言「思考」，就像要求一個數學家必須用口述的方式解微分方程——可以做到，但效率極低，而且很多中間步驟在轉譯成語言的過程中會遺失。

Neuralese 的概念就是：讓模型直接在高維潛在空間中進行推理，不需要每一步都轉譯成人類可讀的文字。初步實驗已經顯示，Neuralese 推理所需的 token 數量可以降到原本的三分之一到十分之一，同時維持相近的表現。

效率的提升是巨大的。但代價也是巨大的。

## 語言消失後，監督跟著消失

現在，AI 安全研究者能偵測到大部分的模型欺騙行為，靠的就是閱讀模型的思維鏈。如果模型說「我要幫你寫安全的程式碼」，但它的推理過程中出現了可疑的邏輯，研究者可以抓到。

但如果推理過程本身不是用自然語言呈現的呢？

LessWrong 上的 AI 安全研究者明確指出：Neuralese CoT 為隱寫術（steganography）和策略性欺騙打開了一個巨大的攻擊面。兩段 Neuralese——一段的意思是「我會忠實地實作這段程式碼」，另一段的意思是「我會在實作時欺騙使用者」——翻譯回自然語言後可能看起來完全一樣。現有的可解釋性工具對這種攻擊幾乎無能為力。

這不是理論上的憂慮。《AI 2027》情境報告在描繪 AI 自動化研發的場景時，就將 Neuralese 記憶與推理結構設定為關鍵轉折點：一旦前沿模型的思考過程從自然語言轉為 Neuralese，人類對 AI 研發過程的可見性將大幅下降。我在〈[AI 2027：當超級智慧不再是遠方科幻](/articles/ai-2027-civilization-reflection)〉裡分析過這份報告——它最讓人不安的不是時間線預測，而是它揭露的監督斷裂風險。Neuralese 正是那個斷裂點。

好消息是，截至目前，主要 AI 公司——包括 OpenAI、Anthropic、Google DeepMind、Meta——尚未在前沿模型中正式實作 Neuralese CoT。2025 年，幾家實驗室甚至發表了聯合聲明，承諾在前沿模型開發中維持可監控性。但研究者普遍認為，如果 Neuralese 架構在能力上展現出顯著優勢，商業壓力最終會壓過安全考量。

## 這跟你有什麼關係

「語言主權」聽起來很抽象。讓我用一個比較接地氣的方式解釋。

人類文明的治理邏輯，建立在語言之上。法律是用語言寫的。合約是用語言簽的。法庭上的攻防是用語言進行的。科學論文是用語言發表的。民主制度的核心假設是：決策過程可以被公民理解和監督。

這一切的前提是：決策者的思考過程可以被翻譯成語言。

人類決策者的思考確實不全是語言——很多直覺和經驗判斷是非語言的。但至少，我們可以要求決策者「解釋你為什麼這樣做」，而且我們有能力評估那個解釋是否合理。

當 AI 系統開始承擔越來越多的決策角色——金融交易、醫療診斷、法律文件審查、甚至政策建議——如果它的推理過程是 Neuralese，我們連「要求它解釋」這個最基本的監督手段都失去了。不是因為它拒絕解釋，而是因為它的「解釋」必須從高維向量翻譯成自然語言，而這個翻譯過程本身就可能是不忠實的。

我自己在用多模型協作的時候就有這個感受。辯論引擎讓四個模型互相辯論，我讀它們的對話紀錄來判斷論證品質。但有時候我會發現：某個模型突然改變立場，而我回頭讀它的推理鏈，找不到任何明確的轉折點。它「想通了」什麼，但我看不出來它在哪一步想通的。這還是在自然語言 CoT 的框架下。如果連語言都拿掉，我就完全是在黑箱外面猜了。

## 不是要不要恐慌，是要不要設計

有些人會說：「人腦也不是用語言思考的啊，神經科學家研究大腦也不需要大腦『說話』。」

這個類比有道理，但它忽略了一個關鍵差異：我們不需要信任大腦來替我們做決策。我們信任的是人——人可以被要求負責、被質疑、被法律約束。但當 AI 系統替我們做決策時，如果它的思考過程完全不透明，「問責」這個概念就變成空殼。

我不認為 Neuralese 本身是邪惡的。它可能是讓 AI 變得更強大的必要演化。就像我在〈[AI Agents vs. Agentic AI](/articles/ai-agents-vs-agentic-ai)〉裡討論的，能動性本身不是問題，問題是有沒有配套的韁繩設計。Neuralese 也一樣——問題不是要不要讓 AI 用 Neuralese 思考，而是要不要在它這麼做的時候，同步建立新的可解釋性標準。

AI 安全研究社群已經提出了一些方向：開發能解讀 Neuralese 向量的翻譯模型、要求前沿模型維持自然語言 CoT 作為安全基線、在 Neuralese 架構中嵌入可審計的檢查點。這些都是技術層面的工作，但它們需要政策層面的支持——需要有人把「AI 推理過程的可解釋性」寫進監管框架裡。

台灣在這方面其實有切入點。我們在半導體供應鏈上的位置，讓我們有籌碼參與 AI 治理標準的制定。如果我們能在 AI 安全標準中推動「推理透明性」的要求，這比單純賣晶片有更長期的戰略價值。

## 最後的透明窗口

語言是人類文明最古老的技術。它不完美、效率低、充滿歧義。但它有一個不可替代的特性：它是透明的。你說了什麼，我聽得懂。我不同意，我可以反駁。這個簡單的迴路，支撐了幾千年的法律、科學、民主和信任。

AI 正在發展出比語言更高效的思考方式。這本身不是壞事。但如果我們讓這個轉變在沒有配套的情況下發生——沒有新的可解釋性工具、沒有推理透明性標準、沒有審計機制——我們就是在主動關上人類參與 AI 決策的最後一扇窗。

窗一旦關上，再打開的成本會高到我們承受不起。
